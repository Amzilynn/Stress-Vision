{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN44xGQWUrM22yNDHe7XeLx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amzilynn/Stress-Vision/blob/main/StressVision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe opencv-python matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "1P3wJaQ3DNLZ",
        "outputId": "051e8dd3-820f-4e69-9222-75fa6a86392c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Using cached mediapipe-0.10.31-py3-none-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py~=2.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (2.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mediapipe) (2.0.2)\n",
            "Requirement already satisfied: sounddevice~=0.5 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: flatbuffers~=25.9 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.9.23)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice~=0.5->mediapipe) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice~=0.5->mediapipe) (2.23)\n",
            "Using cached mediapipe-0.10.31-py3-none-manylinux_2_28_x86_64.whl (10.3 MB)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.10.31\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "mediapipe"
                ]
              },
              "id": "b6c6bb8736054a738dbe440eef96cfed"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_lite/float16/1/pose_landmarker_lite.task"
      ],
      "metadata": {
        "id": "JNeYNmDNG6fT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0bZZk7A4PqC",
        "outputId": "e140c8ea-a3cc-49d1-d019-ac9f37638b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MediaPipe version: 0.10.31\n",
            "Checking for required models...\n",
            "face_landmarker.task already exists\n",
            "hand_landmarker.task already exists\n",
            "Downloading pose_landmarker model...\n",
            "Error downloading pose_landmarker: HTTP Error 404: Not Found\n",
            "Please download manually from: https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker/heavy/latest/pose_landmarker_heavy.task\n",
            "Error initializing MediaPipe models: Unable to open file at pose_landmarker.task\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "from mediapipe import tasks\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "\n",
        "print(f\"MediaPipe version: {mp.__version__}\")\n",
        "\n",
        "def calculate_stress(blink, eyebrow, emotions, lips, hand_movement, gaze_direction, face_orientation):\n",
        "    \"\"\"Calculate overall stress from individual metrics\"\"\"\n",
        "    final_stress = (\n",
        "        0.15 * blink +\n",
        "        0.15 * eyebrow +\n",
        "        0.15 * emotions +\n",
        "        0.15 * lips +\n",
        "        0.15 * hand_movement +\n",
        "        0.15 * gaze_direction +\n",
        "        0.10 * face_orientation\n",
        "    )\n",
        "    return final_stress\n",
        "\n",
        "def calculate_hand_movement(hand_landmarks):\n",
        "    \"\"\"Calculate total hand movement based on landmark distances\"\"\"\n",
        "    if hand_landmarks is None or len(hand_landmarks) == 0:\n",
        "        return 0\n",
        "\n",
        "    landmarks = hand_landmarks[0]  # Get first hand\n",
        "    total_distance = 0\n",
        "\n",
        "    for i in range(1, len(landmarks)):\n",
        "        x1, y1, z1 = landmarks[i-1].x, landmarks[i-1].y, landmarks[i-1].z\n",
        "        x2, y2, z2 = landmarks[i].x, landmarks[i].y, landmarks[i].z\n",
        "        distance = math.sqrt((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)\n",
        "        total_distance += distance\n",
        "\n",
        "    return min(total_distance * 10, 100)  # Normalize to 0-100\n",
        "\n",
        "def get_landmark_safe(landmarks, index):\n",
        "    \"\"\"Safely get a landmark by index\"\"\"\n",
        "    try:\n",
        "        if landmarks and index < len(landmarks):\n",
        "            return landmarks[index]\n",
        "    except (IndexError, AttributeError, TypeError):\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "def analyze_blink(face_landmarks, frame_height):\n",
        "    \"\"\"Analyze eye aspect ratio to detect blinks\"\"\"\n",
        "    if face_landmarks is None or len(face_landmarks) == 0:\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        landmarks = face_landmarks[0]  # Get first face\n",
        "\n",
        "        # Left eye landmarks\n",
        "        left_eye_top = get_landmark_safe(landmarks, 159)\n",
        "        left_eye_bottom = get_landmark_safe(landmarks, 145)\n",
        "        left_eye_left = get_landmark_safe(landmarks, 33)\n",
        "        left_eye_right = get_landmark_safe(landmarks, 133)\n",
        "\n",
        "        if None in [left_eye_top, left_eye_bottom, left_eye_left, left_eye_right]:\n",
        "            return 0\n",
        "\n",
        "        # Calculate eye aspect ratio\n",
        "        vertical_dist = abs(left_eye_top.y - left_eye_bottom.y)\n",
        "        horizontal_dist = abs(left_eye_left.x - left_eye_right.x)\n",
        "\n",
        "        if horizontal_dist > 0:\n",
        "            ear = vertical_dist / horizontal_dist\n",
        "            blink_score = max(0, (0.2 - ear) * 500)\n",
        "            return min(blink_score, 100)\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "    return 0\n",
        "\n",
        "def analyze_eyebrow(face_landmarks):\n",
        "    \"\"\"Analyze eyebrow position and movement\"\"\"\n",
        "    if face_landmarks is None or len(face_landmarks) == 0:\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        landmarks = face_landmarks[0]\n",
        "\n",
        "        left_eyebrow_inner = get_landmark_safe(landmarks, 70)\n",
        "        left_eye = get_landmark_safe(landmarks, 159)\n",
        "        right_eyebrow_inner = get_landmark_safe(landmarks, 300)\n",
        "        right_eye = get_landmark_safe(landmarks, 386)\n",
        "\n",
        "        if None in [left_eyebrow_inner, left_eye, right_eyebrow_inner, right_eye]:\n",
        "            return 0\n",
        "\n",
        "        left_height = abs(left_eyebrow_inner.y - left_eye.y)\n",
        "        right_height = abs(right_eyebrow_inner.y - right_eye.y)\n",
        "        avg_height = (left_height + right_height) / 2\n",
        "\n",
        "        eyebrow_score = max(0, (avg_height - 0.03) * 1000)\n",
        "        return min(eyebrow_score, 100)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return 0\n",
        "\n",
        "def analyze_emotions(face_landmarks):\n",
        "    \"\"\"Analyze facial expressions for emotion indicators\"\"\"\n",
        "    if face_landmarks is None or len(face_landmarks) == 0:\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        landmarks = face_landmarks[0]\n",
        "\n",
        "        left_mouth = get_landmark_safe(landmarks, 61)\n",
        "        right_mouth = get_landmark_safe(landmarks, 291)\n",
        "        mouth_center = get_landmark_safe(landmarks, 13)\n",
        "\n",
        "        if None in [left_mouth, right_mouth, mouth_center]:\n",
        "            return 0\n",
        "\n",
        "        left_curve = left_mouth.y - mouth_center.y\n",
        "        right_curve = right_mouth.y - mouth_center.y\n",
        "        avg_curve = (left_curve + right_curve) / 2\n",
        "\n",
        "        emotion_score = max(0, avg_curve * 500)\n",
        "        return min(emotion_score, 100)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return 0\n",
        "\n",
        "def analyze_lips(face_landmarks):\n",
        "    \"\"\"Analyze lip compression and tension\"\"\"\n",
        "    if face_landmarks is None or len(face_landmarks) == 0:\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        landmarks = face_landmarks[0]\n",
        "\n",
        "        upper_lip = get_landmark_safe(landmarks, 13)\n",
        "        lower_lip = get_landmark_safe(landmarks, 14)\n",
        "\n",
        "        if None in [upper_lip, lower_lip]:\n",
        "            return 0\n",
        "\n",
        "        lip_distance = abs(upper_lip.y - lower_lip.y)\n",
        "        lip_score = max(0, (0.02 - lip_distance) * 2000)\n",
        "        return min(lip_score, 100)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return 0\n",
        "\n",
        "def analyze_gaze_direction(face_landmarks):\n",
        "    \"\"\"Analyze gaze direction and stability\"\"\"\n",
        "    if face_landmarks is None or len(face_landmarks) == 0:\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        landmarks = face_landmarks[0]\n",
        "\n",
        "        left_iris = get_landmark_safe(landmarks, 468)\n",
        "        left_eye_center = get_landmark_safe(landmarks, 33)\n",
        "\n",
        "        if None in [left_iris, left_eye_center]:\n",
        "            return 0\n",
        "\n",
        "        gaze_deviation = abs(left_iris.x - left_eye_center.x)\n",
        "        gaze_score = min(gaze_deviation * 500, 100)\n",
        "        return gaze_score\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return 0\n",
        "\n",
        "def analyze_face_orientation(pose_landmarks):\n",
        "    \"\"\"Analyze head pose and orientation\"\"\"\n",
        "    if pose_landmarks is None or len(pose_landmarks) == 0:\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        landmarks = pose_landmarks[0]\n",
        "\n",
        "        nose = get_landmark_safe(landmarks, 0)\n",
        "        left_shoulder = get_landmark_safe(landmarks, 11)\n",
        "        right_shoulder = get_landmark_safe(landmarks, 12)\n",
        "\n",
        "        if None in [nose, left_shoulder, right_shoulder]:\n",
        "            return 0\n",
        "\n",
        "        left_dist = abs(nose.x - left_shoulder.x)\n",
        "        right_dist = abs(nose.x - right_shoulder.x)\n",
        "        asymmetry = abs(left_dist - right_dist)\n",
        "\n",
        "        orientation_score = min(asymmetry * 200, 100)\n",
        "        return orientation_score\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return 0\n",
        "\n",
        "def download_models():\n",
        "    \"\"\"Download required MediaPipe models\"\"\"\n",
        "    import urllib.request\n",
        "    import os\n",
        "\n",
        "    models = {\n",
        "        'face_landmarker': 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/latest/face_landmarker.task',\n",
        "        'hand_landmarker': 'https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/latest/hand_landmarker.task',\n",
        "    }\n",
        "\n",
        "    for name, url in models.items():\n",
        "        filename = f\"{name}.task\"\n",
        "        if not os.path.exists(filename):\n",
        "            print(f\"Downloading {name} model...\")\n",
        "            try:\n",
        "                urllib.request.urlretrieve(url, filename)\n",
        "                print(f\"Downloaded {filename}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading {name}: {e}\")\n",
        "                print(f\"Please download manually from: {url}\")\n",
        "                return False\n",
        "        else:\n",
        "            print(f\"{filename} already exists\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def analyze_video(video_path):\n",
        "    \"\"\"Main function to analyze video and extract stress metrics\"\"\"\n",
        "\n",
        "    # Download models if needed\n",
        "    print(\"Checking for required models...\")\n",
        "    download_models()\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error: Could not open video file {video_path}\")\n",
        "        return None\n",
        "\n",
        "    # Initialize MediaPipe models\n",
        "    try:\n",
        "        base_options_face = python.BaseOptions(model_asset_path='face_landmarker.task')\n",
        "        options_face = vision.FaceLandmarkerOptions(\n",
        "            base_options=base_options_face,\n",
        "            output_face_blendshapes=False,\n",
        "            output_facial_transformation_matrixes=False,\n",
        "            num_faces=1\n",
        "        )\n",
        "        face_detector = vision.FaceLandmarker.create_from_options(options_face)\n",
        "\n",
        "        base_options_hand = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "        options_hand = vision.HandLandmarkerOptions(\n",
        "            base_options=base_options_hand,\n",
        "            num_hands=2\n",
        "        )\n",
        "        hand_detector = vision.HandLandmarker.create_from_options(options_hand)\n",
        "\n",
        "        \"\"\"base_options_pose = python.BaseOptions(model_asset_path='pose_landmarker.task')\"\"\"\n",
        "        options_pose = vision.PoseLandmarkerOptions(\n",
        "            base_options=base_options_pose,\n",
        "            output_segmentation_masks=False\n",
        "        )\n",
        "        pose_detector = vision.PoseLandmarker.create_from_options(options_pose)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing MediaPipe models: {e}\")\n",
        "        return None\n",
        "\n",
        "    stress_data = {\n",
        "        'blink': [],\n",
        "        'eyebrow': [],\n",
        "        'emotions': [],\n",
        "        'lips': [],\n",
        "        'hand_movement': [],\n",
        "        'gaze_direction': [],\n",
        "        'face_orientation': [],\n",
        "        'overall': []\n",
        "    }\n",
        "\n",
        "    frame_count = 0\n",
        "    print(\"Processing video...\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"Processing frame {frame_count}...\")\n",
        "\n",
        "        try:\n",
        "            # Convert to RGB\n",
        "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            frame_height = frame.shape[0]\n",
        "\n",
        "            # Create MediaPipe Image\n",
        "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
        "\n",
        "            # Detect landmarks\n",
        "            face_result = face_detector.detect(mp_image)\n",
        "            hand_result = hand_detector.detect(mp_image)\n",
        "            pose_result = pose_detector.detect(mp_image)\n",
        "\n",
        "            # Extract metrics\n",
        "            blink = analyze_blink(face_result.face_landmarks, frame_height)\n",
        "            eyebrow = analyze_eyebrow(face_result.face_landmarks)\n",
        "            emotions = analyze_emotions(face_result.face_landmarks)\n",
        "            lips = analyze_lips(face_result.face_landmarks)\n",
        "            hand_movement = calculate_hand_movement(hand_result.hand_landmarks)\n",
        "            gaze_direction = analyze_gaze_direction(face_result.face_landmarks)\n",
        "            face_orientation = analyze_face_orientation(pose_result.pose_landmarks)\n",
        "\n",
        "            # Store data\n",
        "            stress_data['blink'].append(blink)\n",
        "            stress_data['eyebrow'].append(eyebrow)\n",
        "            stress_data['emotions'].append(emotions)\n",
        "            stress_data['lips'].append(lips)\n",
        "            stress_data['hand_movement'].append(hand_movement)\n",
        "            stress_data['gaze_direction'].append(gaze_direction)\n",
        "            stress_data['face_orientation'].append(face_orientation)\n",
        "\n",
        "            # Calculate overall stress\n",
        "            overall_stress = calculate_stress(\n",
        "                blink, eyebrow, emotions, lips,\n",
        "                hand_movement, gaze_direction, face_orientation\n",
        "            )\n",
        "            stress_data['overall'].append(overall_stress)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing frame {frame_count}: {e}\")\n",
        "            continue\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Video analysis complete! Processed {frame_count} frames.\")\n",
        "    return stress_data\n",
        "\n",
        "def plot_graph(stress_data):\n",
        "    \"\"\"Generate and save stress visualization\"\"\"\n",
        "    if not stress_data or len(stress_data['overall']) == 0:\n",
        "        print(\"No data to plot!\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "    # Plot overall stress\n",
        "    axes[0].plot(stress_data['overall'], linewidth=2, color='red')\n",
        "    axes[0].set_title('Overall Stress Levels Over Time', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('Frame Number')\n",
        "    axes[0].set_ylabel('Stress Level (0-100)')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_ylim([0, 100])\n",
        "\n",
        "    # Plot individual metrics\n",
        "    axes[1].plot(stress_data['blink'], label='Blink', alpha=0.7)\n",
        "    axes[1].plot(stress_data['eyebrow'], label='Eyebrow', alpha=0.7)\n",
        "    axes[1].plot(stress_data['emotions'], label='Emotions', alpha=0.7)\n",
        "    axes[1].plot(stress_data['lips'], label='Lips', alpha=0.7)\n",
        "    axes[1].plot(stress_data['hand_movement'], label='Hand Movement', alpha=0.7)\n",
        "    axes[1].plot(stress_data['gaze_direction'], label='Gaze', alpha=0.7)\n",
        "    axes[1].plot(stress_data['face_orientation'], label='Face Orientation', alpha=0.7)\n",
        "\n",
        "    axes[1].set_title('Individual Stress Metrics', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('Frame Number')\n",
        "    axes[1].set_ylabel('Metric Value (0-100)')\n",
        "    axes[1].legend(loc='upper right')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].set_ylim([0, 100])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('stress_graph.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Graph saved as 'stress_graph.png'\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\n=== Stress Analysis Statistics ===\")\n",
        "    print(f\"Average Overall Stress: {np.mean(stress_data['overall']):.2f}\")\n",
        "    print(f\"Maximum Stress: {np.max(stress_data['overall']):.2f}\")\n",
        "    print(f\"Minimum Stress: {np.min(stress_data['overall']):.2f}\")\n",
        "    print(f\"Standard Deviation: {np.std(stress_data['overall']):.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Update this path to your video file\n",
        "    video_path = \"/content/vd.mp4\"\n",
        "\n",
        "    # Analyze video\n",
        "    stress_data = analyze_video(video_path)\n",
        "\n",
        "    # Generate graph\n",
        "    if stress_data:\n",
        "        plot_graph(stress_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"MediaPipe version: {mp.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKzpMC-CEBmk",
        "outputId": "2d1cf415-1490-42c0-cdef-ef7ff7ad4ea4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MediaPipe version: 0.10.31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "print(dir(mp))  # list all attributes of mp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBcX3s_BERXo",
        "outputId": "9aa52091-cb23-47fe-e088-a7bb0da400df"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Image', 'ImageFormat', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', 'tasks']\n"
          ]
        }
      ]
    }
  ]
}